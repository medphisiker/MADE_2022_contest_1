{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание конкурса\n",
    "\n",
    "Машинное обучение: часть 1\n",
    "\n",
    "Сроки проведения: 07.08.22 - 22.08.22\n",
    "\n",
    "До конца осталось:13 дней:22 часа\n",
    "\n",
    "Баллы за каждую задачу вычисляются на основе загруженных предсказаний для соответствующего тестового набора данных. В обеих задачах тестовый набор предварительно разделен на две части: публичную (public, 45%) и приватную (private, 55%). Каждый раз после загрузки решения участнику отображаются баллы, посчитанные на публичной части. Они не видны другим участникам соревнования. Далее после завершения соревнования в качестве итоговых баллов за задачу берутся баллы, посчитанные на соответствующей приватной части в лучшем загруженном участником решении.\n",
    "\n",
    "Обращаем внимание, что участник должен отправить предсказания для всего тестового набора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рабочее окружение\n",
    "\n",
    "Для библиотеки Sber LAMA требуется специфические версии PyTorch и других библиотек. Лучше создть conda отдельное рабочее окружение:\n",
    "\n",
    "    conda create -n sber_lama python=3.9.7\n",
    "\n",
    "с той же версией python, что и в окружении base. И уже потом находясь в этом окружении выполнить команду по устновке lighautoml:\n",
    "\n",
    "    !pip install -U lightautoml\n",
    "\n",
    "Менеджер пакетов сам установит все нужные зависисмоти нужных версий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 1\n",
      "initial apicid\t: 1\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 2\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 2\n",
      "initial apicid\t: 2\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 3\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 3\n",
      "initial apicid\t: 3\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 4\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 2\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 4\n",
      "initial apicid\t: 4\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 5\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 2\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 5\n",
      "initial apicid\t: 5\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 6\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 3\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 6\n",
      "initial apicid\t: 6\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 7\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 3\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 7\n",
      "initial apicid\t: 7\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 8\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 4283.096\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 4\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 8\n",
      "initial apicid\t: 8\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 9\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 4\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 9\n",
      "initial apicid\t: 9\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 10\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 4785.005\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 5\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 10\n",
      "initial apicid\t: 10\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 11\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 151\n",
      "model name\t: 12th Gen Intel(R) Core(TM) i5-12600\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1f\n",
      "cpu MHz\t\t: 3300.000\n",
      "cache size\t: 18432 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 12\n",
      "core id\t\t: 5\n",
      "cpu cores\t: 6\n",
      "apicid\t\t: 11\n",
      "initial apicid\t: 11\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 32\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\n",
      "vmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling usr_wait_pause\n",
      "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs\n",
      "bogomips\t: 6604.80\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 10 16:29:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          Off  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   45C    P0    56W / 150W |      0MiB / 24564MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "При выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "# Завершить все процессы, использующие ГПУ в нашей ОС\n",
    "# !fuser -k /dev/nvidia[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установим AutoML фреймворк от Сбера - LightAutoML(LAMA)\n",
    "# это позволит довольно быстро проверять различные гипотезы и \n",
    "# отбирать лучшие решения\n",
    "# (устанавливаем только 1 раз)\n",
    "# !pip install -U lightautoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-gpu/anaconda3/envs/sber_lama/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Импортируем модули\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "# уже установленные библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# импорты из LightAutoML\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.130724</td>\n",
       "      <td>7.429628</td>\n",
       "      <td>3.651437</td>\n",
       "      <td>-1.950971</td>\n",
       "      <td>-3.384415</td>\n",
       "      <td>1.095934</td>\n",
       "      <td>-3.077774</td>\n",
       "      <td>-2.619091</td>\n",
       "      <td>5.128201</td>\n",
       "      <td>1.228476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392978</td>\n",
       "      <td>4.920467</td>\n",
       "      <td>16.610460</td>\n",
       "      <td>-2.430804</td>\n",
       "      <td>-1.405573</td>\n",
       "      <td>18.610209</td>\n",
       "      <td>-4.140715</td>\n",
       "      <td>6.027816</td>\n",
       "      <td>-20.288145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.161836</td>\n",
       "      <td>-6.514011</td>\n",
       "      <td>-7.474612</td>\n",
       "      <td>-4.544336</td>\n",
       "      <td>-13.081404</td>\n",
       "      <td>1.637562</td>\n",
       "      <td>-1.094672</td>\n",
       "      <td>-1.253545</td>\n",
       "      <td>-2.955342</td>\n",
       "      <td>-10.958200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840233</td>\n",
       "      <td>-0.514308</td>\n",
       "      <td>4.613289</td>\n",
       "      <td>2.391302</td>\n",
       "      <td>-4.795664</td>\n",
       "      <td>4.208278</td>\n",
       "      <td>-2.017168</td>\n",
       "      <td>-8.510424</td>\n",
       "      <td>10.806639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.971550</td>\n",
       "      <td>-21.508184</td>\n",
       "      <td>-1.125457</td>\n",
       "      <td>1.524129</td>\n",
       "      <td>3.027444</td>\n",
       "      <td>1.045879</td>\n",
       "      <td>1.551050</td>\n",
       "      <td>1.512075</td>\n",
       "      <td>-1.955564</td>\n",
       "      <td>3.683893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.385303</td>\n",
       "      <td>2.647917</td>\n",
       "      <td>-2.200556</td>\n",
       "      <td>1.058671</td>\n",
       "      <td>1.076312</td>\n",
       "      <td>-7.802389</td>\n",
       "      <td>-7.553953</td>\n",
       "      <td>0.636639</td>\n",
       "      <td>14.274950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.724419</td>\n",
       "      <td>0.566489</td>\n",
       "      <td>0.509764</td>\n",
       "      <td>-4.524162</td>\n",
       "      <td>10.367236</td>\n",
       "      <td>2.083270</td>\n",
       "      <td>0.741790</td>\n",
       "      <td>-2.077787</td>\n",
       "      <td>-2.912744</td>\n",
       "      <td>-4.040637</td>\n",
       "      <td>...</td>\n",
       "      <td>4.731346</td>\n",
       "      <td>15.378418</td>\n",
       "      <td>-14.031666</td>\n",
       "      <td>2.659410</td>\n",
       "      <td>5.123620</td>\n",
       "      <td>-8.500321</td>\n",
       "      <td>3.417960</td>\n",
       "      <td>-14.798490</td>\n",
       "      <td>-6.132800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.213659</td>\n",
       "      <td>-4.678213</td>\n",
       "      <td>-0.135845</td>\n",
       "      <td>2.375933</td>\n",
       "      <td>0.916649</td>\n",
       "      <td>1.027195</td>\n",
       "      <td>-0.353265</td>\n",
       "      <td>-0.220609</td>\n",
       "      <td>-3.416823</td>\n",
       "      <td>-5.964181</td>\n",
       "      <td>...</td>\n",
       "      <td>1.598330</td>\n",
       "      <td>-4.996614</td>\n",
       "      <td>4.504269</td>\n",
       "      <td>1.918961</td>\n",
       "      <td>-2.076223</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>-2.016779</td>\n",
       "      <td>10.803205</td>\n",
       "      <td>5.942927</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1         2         3          4         5         6  \\\n",
       "0 -6.130724   7.429628  3.651437 -1.950971  -3.384415  1.095934 -3.077774   \n",
       "1  5.161836  -6.514011 -7.474612 -4.544336 -13.081404  1.637562 -1.094672   \n",
       "2 -2.971550 -21.508184 -1.125457  1.524129   3.027444  1.045879  1.551050   \n",
       "3  6.724419   0.566489  0.509764 -4.524162  10.367236  2.083270  0.741790   \n",
       "4 -2.213659  -4.678213 -0.135845  2.375933   0.916649  1.027195 -0.353265   \n",
       "\n",
       "          7         8          9  ...        22         23         24  \\\n",
       "0 -2.619091  5.128201   1.228476  ...  0.392978   4.920467  16.610460   \n",
       "1 -1.253545 -2.955342 -10.958200  ... -4.840233  -0.514308   4.613289   \n",
       "2  1.512075 -1.955564   3.683893  ... -0.385303   2.647917  -2.200556   \n",
       "3 -2.077787 -2.912744  -4.040637  ...  4.731346  15.378418 -14.031666   \n",
       "4 -0.220609 -3.416823  -5.964181  ...  1.598330  -4.996614   4.504269   \n",
       "\n",
       "         25        26         27        28         29         30  target  \n",
       "0 -2.430804 -1.405573  18.610209 -4.140715   6.027816 -20.288145       0  \n",
       "1  2.391302 -4.795664   4.208278 -2.017168  -8.510424  10.806639       0  \n",
       "2  1.058671  1.076312  -7.802389 -7.553953   0.636639  14.274950       0  \n",
       "3  2.659410  5.123620  -8.500321  3.417960 -14.798490  -6.132800       1  \n",
       "4  1.918961 -2.076223   0.154039 -2.016779  10.803205   5.942927       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим обучающие данные и посмотрим на них.\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.768975</td>\n",
       "      <td>-1.744984</td>\n",
       "      <td>0.069141</td>\n",
       "      <td>-6.560182</td>\n",
       "      <td>13.267557</td>\n",
       "      <td>1.768138</td>\n",
       "      <td>1.711161</td>\n",
       "      <td>-5.289421</td>\n",
       "      <td>2.682271</td>\n",
       "      <td>-2.763065</td>\n",
       "      <td>...</td>\n",
       "      <td>4.198756</td>\n",
       "      <td>-0.625212</td>\n",
       "      <td>11.053370</td>\n",
       "      <td>1.535982</td>\n",
       "      <td>0.410761</td>\n",
       "      <td>-3.754269</td>\n",
       "      <td>8.633258</td>\n",
       "      <td>0.224213</td>\n",
       "      <td>-2.310344</td>\n",
       "      <td>-20.482680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.085184</td>\n",
       "      <td>7.679585</td>\n",
       "      <td>-2.392444</td>\n",
       "      <td>-1.334928</td>\n",
       "      <td>-17.347661</td>\n",
       "      <td>2.470484</td>\n",
       "      <td>-0.662081</td>\n",
       "      <td>-0.719648</td>\n",
       "      <td>-2.489930</td>\n",
       "      <td>-11.762400</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.176409</td>\n",
       "      <td>1.365061</td>\n",
       "      <td>0.140867</td>\n",
       "      <td>-4.210869</td>\n",
       "      <td>-5.718286</td>\n",
       "      <td>-4.055603</td>\n",
       "      <td>9.264081</td>\n",
       "      <td>-4.221163</td>\n",
       "      <td>7.481250</td>\n",
       "      <td>9.426657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.498828</td>\n",
       "      <td>-3.945847</td>\n",
       "      <td>-1.647106</td>\n",
       "      <td>0.058655</td>\n",
       "      <td>1.204024</td>\n",
       "      <td>1.043502</td>\n",
       "      <td>5.246244</td>\n",
       "      <td>2.123335</td>\n",
       "      <td>2.851375</td>\n",
       "      <td>4.398413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301051</td>\n",
       "      <td>0.758360</td>\n",
       "      <td>0.090010</td>\n",
       "      <td>-1.550171</td>\n",
       "      <td>-0.971835</td>\n",
       "      <td>2.027758</td>\n",
       "      <td>-2.723663</td>\n",
       "      <td>0.749390</td>\n",
       "      <td>-12.130969</td>\n",
       "      <td>4.706467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.813839</td>\n",
       "      <td>-1.913776</td>\n",
       "      <td>1.308982</td>\n",
       "      <td>3.866011</td>\n",
       "      <td>-9.249616</td>\n",
       "      <td>2.265848</td>\n",
       "      <td>-3.257832</td>\n",
       "      <td>-2.233508</td>\n",
       "      <td>3.623846</td>\n",
       "      <td>4.665100</td>\n",
       "      <td>...</td>\n",
       "      <td>2.316560</td>\n",
       "      <td>-0.590067</td>\n",
       "      <td>-6.825693</td>\n",
       "      <td>8.512672</td>\n",
       "      <td>-3.259642</td>\n",
       "      <td>-2.297466</td>\n",
       "      <td>4.758336</td>\n",
       "      <td>-2.134407</td>\n",
       "      <td>10.448348</td>\n",
       "      <td>-5.808233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.098517</td>\n",
       "      <td>1.831659</td>\n",
       "      <td>2.587683</td>\n",
       "      <td>1.507983</td>\n",
       "      <td>5.421837</td>\n",
       "      <td>2.184649</td>\n",
       "      <td>1.214167</td>\n",
       "      <td>0.402367</td>\n",
       "      <td>1.256427</td>\n",
       "      <td>17.582508</td>\n",
       "      <td>...</td>\n",
       "      <td>20.077786</td>\n",
       "      <td>-2.657541</td>\n",
       "      <td>5.592968</td>\n",
       "      <td>1.339930</td>\n",
       "      <td>0.425855</td>\n",
       "      <td>2.649136</td>\n",
       "      <td>-6.496886</td>\n",
       "      <td>-0.726197</td>\n",
       "      <td>-12.478830</td>\n",
       "      <td>1.058612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3          4         5         6  \\\n",
       "0 -4.768975 -1.744984  0.069141 -6.560182  13.267557  1.768138  1.711161   \n",
       "1  3.085184  7.679585 -2.392444 -1.334928 -17.347661  2.470484 -0.662081   \n",
       "2 -1.498828 -3.945847 -1.647106  0.058655   1.204024  1.043502  5.246244   \n",
       "3 -2.813839 -1.913776  1.308982  3.866011  -9.249616  2.265848 -3.257832   \n",
       "4 -2.098517  1.831659  2.587683  1.507983   5.421837  2.184649  1.214167   \n",
       "\n",
       "          7         8          9  ...         21        22         23  \\\n",
       "0 -5.289421  2.682271  -2.763065  ...   4.198756 -0.625212  11.053370   \n",
       "1 -0.719648 -2.489930 -11.762400  ...  -1.176409  1.365061   0.140867   \n",
       "2  2.123335  2.851375   4.398413  ...   0.301051  0.758360   0.090010   \n",
       "3 -2.233508  3.623846   4.665100  ...   2.316560 -0.590067  -6.825693   \n",
       "4  0.402367  1.256427  17.582508  ...  20.077786 -2.657541   5.592968   \n",
       "\n",
       "         24        25        26        27        28         29         30  \n",
       "0  1.535982  0.410761 -3.754269  8.633258  0.224213  -2.310344 -20.482680  \n",
       "1 -4.210869 -5.718286 -4.055603  9.264081 -4.221163   7.481250   9.426657  \n",
       "2 -1.550171 -0.971835  2.027758 -2.723663  0.749390 -12.130969   4.706467  \n",
       "3  8.512672 -3.259642 -2.297466  4.758336 -2.134407  10.448348  -5.808233  \n",
       "4  1.339930  0.425855  2.649136 -6.496886 -0.726197 -12.478830   1.058612  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим тестовые данные и посмотрим на них.\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим пример предсказаний и посмотрим на них.\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts sizes: tr_data = (5570, 32), valid_data = (1393, 32)\n"
     ]
    }
   ],
   "source": [
    "# Разобьем выборку на обучающую и валидационную\n",
    "tr_data, valid_data = train_test_split(train_data, \n",
    "                                       test_size=0.2, \n",
    "                                       stratify=train_data['target'], \n",
    "                                       random_state=42)\n",
    "\n",
    "print(f'Parts sizes: tr_data = {tr_data.shape}, valid_data = {valid_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "30        0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**  пропусков в данных нет! Отлично =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4963, 2000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data['target'].values, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследуем подходы и модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ставим задачу машинного обучения используем все"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightautoml.readthedocs.io/en/latest/python_api/generated/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task\n",
    "task = Task('binary', loss= 'logloss', metric = 'logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не похоже чтобы колонки были категориальными оставляем просто как есть и пусть LAMA сама разберется =)\n",
    "Исключать какие либо колонки из рассмотрения тоже пока не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {'target': 'target'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(task = task, \n",
    "                    timeout = 3600 * 3, # 3600 секунд = 1 час\n",
    "                    general_params = {'use_algos': [['cb', 'lgb', 'linear_l2']]},\n",
    "                    cb_params = {'default_params': {'task_type': 'GPU'}},\n",
    "                    reader_params = {'n_jobs': 12},\n",
    "                    timing_params ={'mode': 0}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:27:24] Stdout logging level is DEBUG.\n",
      "[10:27:24] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[10:27:24] Task: binary\n",
      "\n",
      "[10:27:24] Start automl preset with listed constraints:\n",
      "[10:27:24] - time: 10800.00 seconds\n",
      "[10:27:24] - CPU: 4 cores\n",
      "[10:27:24] - memory: 16 GB\n",
      "\n",
      "[10:27:24] \u001b[1mTrain data shape: (5570, 32)\u001b[0m\n",
      "\n",
      "[10:27:25] Feats was rejected during automatic roles guess: []\n",
      "[10:27:25] Layer \u001b[1m1\u001b[0m train process start. Time left 10799.27 secs\n",
      "[10:27:25] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[10:27:25] Training params: {'tol': 1e-06, 'max_iter': 100, 'cs': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000], 'early_stopping': 2, 'categorical_idx': [], 'embed_sizes': (), 'data_size': 31}\n",
      "[10:27:25] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:27:25] Linear model: C = 1e-05 score = -0.5863713572601014\n",
      "[10:27:25] Linear model: C = 5e-05 score = -0.5402486143463175\n",
      "[10:27:25] Linear model: C = 0.0001 score = -0.49489417835347743\n",
      "[10:27:25] Linear model: C = 0.0005 score = -0.3334994531057681\n",
      "[10:27:25] Linear model: C = 0.001 score = -0.26607367863855197\n",
      "[10:27:25] Linear model: C = 0.005 score = -0.15403766177061284\n",
      "[10:27:25] Linear model: C = 0.01 score = -0.1222620777159497\n",
      "[10:27:25] Linear model: C = 0.05 score = -0.07315935170748793\n",
      "[10:27:25] Linear model: C = 0.1 score = -0.059183724911567814\n",
      "[10:27:25] Linear model: C = 0.5 score = -0.036860317714202444\n",
      "[10:27:25] Linear model: C = 1 score = -0.030532537598755464\n",
      "[10:27:25] Linear model: C = 5 score = -0.019409040420214515\n",
      "[10:27:25] Linear model: C = 10 score = -0.019409040420214515\n",
      "[10:27:25] Linear model: C = 50 score = -0.019409040420214515\n",
      "[10:27:25] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:27:25] Linear model: C = 1e-05 score = -0.5861730263091931\n",
      "[10:27:25] Linear model: C = 5e-05 score = -0.5395691696039436\n",
      "[10:27:25] Linear model: C = 0.0001 score = -0.49363693917300583\n",
      "[10:27:25] Linear model: C = 0.0005 score = -0.32996779435466395\n",
      "[10:27:25] Linear model: C = 0.001 score = -0.26123467376673026\n",
      "[10:27:25] Linear model: C = 0.005 score = -0.14757309448228115\n",
      "[10:27:25] Linear model: C = 0.01 score = -0.11599810590352645\n",
      "[10:27:25] Linear model: C = 0.05 score = -0.06656111206126025\n",
      "[10:27:25] Linear model: C = 0.1 score = -0.05238013146121988\n",
      "[10:27:25] Linear model: C = 0.5 score = -0.02957037097856839\n",
      "[10:27:25] Linear model: C = 1 score = -0.023090338361413106\n",
      "[10:27:25] Linear model: C = 5 score = -0.012946438446860536\n",
      "[10:27:25] Linear model: C = 10 score = -0.012946438446860536\n",
      "[10:27:25] Linear model: C = 50 score = -0.012946438446860536\n",
      "[10:27:25] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:27:25] Linear model: C = 1e-05 score = -0.5863997582580286\n",
      "[10:27:25] Linear model: C = 5e-05 score = -0.5409099023581835\n",
      "[10:27:25] Linear model: C = 0.0001 score = -0.4962112460573015\n",
      "[10:27:25] Linear model: C = 0.0005 score = -0.3358314522613957\n",
      "[10:27:25] Linear model: C = 0.001 score = -0.26859515570639175\n",
      "[10:27:25] Linear model: C = 0.005 score = -0.15545627981828963\n",
      "[10:27:25] Linear model: C = 0.01 score = -0.12319333968108062\n",
      "[10:27:25] Linear model: C = 0.05 score = -0.07333328756548117\n",
      "[10:27:25] Linear model: C = 0.1 score = -0.05921428159707946\n",
      "[10:27:25] Linear model: C = 0.5 score = -0.03721561731837733\n",
      "[10:27:25] Linear model: C = 1 score = -0.03060708691480918\n",
      "[10:27:25] Linear model: C = 5 score = -0.02025701197858312\n",
      "[10:27:25] Linear model: C = 10 score = -0.02025701197858312\n",
      "[10:27:25] Linear model: C = 50 score = -0.02025701197858312\n",
      "[10:27:25] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:27:25] Linear model: C = 1e-05 score = -0.5864816488868673\n",
      "[10:27:25] Linear model: C = 5e-05 score = -0.5408761836253976\n",
      "[10:27:25] Linear model: C = 0.0001 score = -0.49564563022856223\n",
      "[10:27:25] Linear model: C = 0.0005 score = -0.33605038864902576\n",
      "[10:27:25] Linear model: C = 0.001 score = -0.2689155167206132\n",
      "[10:27:25] Linear model: C = 0.005 score = -0.1565038274983663\n",
      "[10:27:25] Linear model: C = 0.01 score = -0.12404562696484102\n",
      "[10:27:25] Linear model: C = 0.05 score = -0.07350955810765988\n",
      "[10:27:25] Linear model: C = 0.1 score = -0.05900681291447309\n",
      "[10:27:26] Linear model: C = 0.5 score = -0.03583666819621784\n",
      "[10:27:26] Linear model: C = 1 score = -0.028826564311438634\n",
      "[10:27:26] Linear model: C = 5 score = -0.018487356990942578\n",
      "[10:27:26] Linear model: C = 10 score = -0.018487356990942578\n",
      "[10:27:26] Linear model: C = 50 score = -0.018487356990942578\n",
      "[10:27:26] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:27:26] Linear model: C = 1e-05 score = -0.5862001513985372\n",
      "[10:27:26] Linear model: C = 5e-05 score = -0.5397478397166493\n",
      "[10:27:26] Linear model: C = 0.0001 score = -0.4932034287327496\n",
      "[10:27:26] Linear model: C = 0.0005 score = -0.32995541023234276\n",
      "[10:27:26] Linear model: C = 0.001 score = -0.26271899236128726\n",
      "[10:27:26] Linear model: C = 0.005 score = -0.15165986004285514\n",
      "[10:27:26] Linear model: C = 0.01 score = -0.12041097211228688\n",
      "[10:27:26] Linear model: C = 0.05 score = -0.0718213006364672\n",
      "[10:27:26] Linear model: C = 0.1 score = -0.057616155957560686\n",
      "[10:27:26] Linear model: C = 0.5 score = -0.03464183529642884\n",
      "[10:27:26] Linear model: C = 1 score = -0.02772724325404043\n",
      "[10:27:26] Linear model: C = 5 score = -0.016414154091494135\n",
      "[10:27:26] Linear model: C = 10 score = -0.016414154091494135\n",
      "[10:27:26] Linear model: C = 50 score = -0.016414154091494135\n",
      "[10:27:26] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m-0.017502800385618977\u001b[0m\n",
      "[10:27:26] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[10:27:26] Time left 10798.49 secs\n",
      "\n",
      "[10:27:26] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:26] [100]\tvalid's binary_logloss: 0.169928\n",
      "[10:27:26] [200]\tvalid's binary_logloss: 0.0613844\n",
      "[10:27:26] [300]\tvalid's binary_logloss: 0.0247968\n",
      "[10:27:26] [400]\tvalid's binary_logloss: 0.0116637\n",
      "[10:27:26] [500]\tvalid's binary_logloss: 0.00694212\n",
      "[10:27:26] [600]\tvalid's binary_logloss: 0.00528527\n",
      "[10:27:26] [700]\tvalid's binary_logloss: 0.00466318\n",
      "[10:27:26] [800]\tvalid's binary_logloss: 0.00448696\n",
      "[10:27:26] [900]\tvalid's binary_logloss: 0.00444627\n",
      "[10:27:26] [1000]\tvalid's binary_logloss: 0.00441204\n",
      "[10:27:26] [1100]\tvalid's binary_logloss: 0.00441204\n",
      "[10:27:26] Early stopping, best iteration is:\n",
      "[980]\tvalid's binary_logloss: 0.00440941\n",
      "[10:27:26] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[10:27:26] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[10:27:26] Training params: {'task': 'train', 'learning_rate': 0.01, 'num_leaves': 32, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 0.5, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 4, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 200, 'random_state': 42}\n",
      "[10:27:26] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:27:26] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:26] [100]\tvalid's binary_logloss: 0.240472\n",
      "[10:27:26] [200]\tvalid's binary_logloss: 0.111099\n",
      "[10:27:26] [300]\tvalid's binary_logloss: 0.0534129\n",
      "[10:27:26] [400]\tvalid's binary_logloss: 0.0269937\n",
      "[10:27:26] [500]\tvalid's binary_logloss: 0.0142413\n",
      "[10:27:26] [600]\tvalid's binary_logloss: 0.00851024\n",
      "[10:27:26] [700]\tvalid's binary_logloss: 0.00556348\n",
      "[10:27:26] [800]\tvalid's binary_logloss: 0.00420106\n",
      "[10:27:26] [900]\tvalid's binary_logloss: 0.0036184\n",
      "[10:27:26] [1000]\tvalid's binary_logloss: 0.00328449\n",
      "[10:27:26] [1100]\tvalid's binary_logloss: 0.00312933\n",
      "[10:27:26] [1200]\tvalid's binary_logloss: 0.00305116\n",
      "[10:27:26] [1300]\tvalid's binary_logloss: 0.00302508\n",
      "[10:27:26] [1400]\tvalid's binary_logloss: 0.00300312\n",
      "[10:27:26] [1500]\tvalid's binary_logloss: 0.00300362\n",
      "[10:27:26] [1600]\tvalid's binary_logloss: 0.00300362\n",
      "[10:27:26] Early stopping, best iteration is:\n",
      "[1467]\tvalid's binary_logloss: 0.00299453\n",
      "[10:27:26] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:27:26] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:26] [100]\tvalid's binary_logloss: 0.239328\n",
      "[10:27:26] [200]\tvalid's binary_logloss: 0.109991\n",
      "[10:27:26] [300]\tvalid's binary_logloss: 0.0523195\n",
      "[10:27:27] [400]\tvalid's binary_logloss: 0.0259828\n",
      "[10:27:27] [500]\tvalid's binary_logloss: 0.0133188\n",
      "[10:27:27] [600]\tvalid's binary_logloss: 0.00770784\n",
      "[10:27:27] [700]\tvalid's binary_logloss: 0.00461519\n",
      "[10:27:27] [800]\tvalid's binary_logloss: 0.00303437\n",
      "[10:27:27] [900]\tvalid's binary_logloss: 0.00241327\n",
      "[10:27:27] [1000]\tvalid's binary_logloss: 0.00204286\n",
      "[10:27:27] [1100]\tvalid's binary_logloss: 0.00184483\n",
      "[10:27:27] [1200]\tvalid's binary_logloss: 0.00174602\n",
      "[10:27:27] [1300]\tvalid's binary_logloss: 0.00166275\n",
      "[10:27:27] [1400]\tvalid's binary_logloss: 0.00160234\n",
      "[10:27:27] [1500]\tvalid's binary_logloss: 0.00161496\n",
      "[10:27:27] [1600]\tvalid's binary_logloss: 0.00161496\n",
      "[10:27:27] Early stopping, best iteration is:\n",
      "[1400]\tvalid's binary_logloss: 0.00160234\n",
      "[10:27:27] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:27:27] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:27] [100]\tvalid's binary_logloss: 0.240341\n",
      "[10:27:27] [200]\tvalid's binary_logloss: 0.111283\n",
      "[10:27:27] [300]\tvalid's binary_logloss: 0.0537155\n",
      "[10:27:27] [400]\tvalid's binary_logloss: 0.0276049\n",
      "[10:27:27] [500]\tvalid's binary_logloss: 0.0150243\n",
      "[10:27:27] [600]\tvalid's binary_logloss: 0.00942275\n",
      "[10:27:27] [700]\tvalid's binary_logloss: 0.00644912\n",
      "[10:27:27] [800]\tvalid's binary_logloss: 0.00497816\n",
      "[10:27:27] [900]\tvalid's binary_logloss: 0.00428468\n",
      "[10:27:27] [1000]\tvalid's binary_logloss: 0.004001\n",
      "[10:27:27] [1100]\tvalid's binary_logloss: 0.00382674\n",
      "[10:27:27] [1200]\tvalid's binary_logloss: 0.00371289\n",
      "[10:27:27] [1300]\tvalid's binary_logloss: 0.00369671\n",
      "[10:27:27] [1400]\tvalid's binary_logloss: 0.00366497\n",
      "[10:27:27] [1500]\tvalid's binary_logloss: 0.00364145\n",
      "[10:27:27] Early stopping, best iteration is:\n",
      "[1382]\tvalid's binary_logloss: 0.00363586\n",
      "[10:27:27] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:27:27] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:27] [100]\tvalid's binary_logloss: 0.239093\n",
      "[10:27:27] [200]\tvalid's binary_logloss: 0.109453\n",
      "[10:27:27] [300]\tvalid's binary_logloss: 0.0518826\n",
      "[10:27:27] [400]\tvalid's binary_logloss: 0.025703\n",
      "[10:27:27] [500]\tvalid's binary_logloss: 0.013088\n",
      "[10:27:27] [600]\tvalid's binary_logloss: 0.00746094\n",
      "[10:27:27] [700]\tvalid's binary_logloss: 0.00451748\n",
      "[10:27:27] [800]\tvalid's binary_logloss: 0.00309617\n",
      "[10:27:27] [900]\tvalid's binary_logloss: 0.00247966\n",
      "[10:27:28] [1000]\tvalid's binary_logloss: 0.00219167\n",
      "[10:27:28] [1100]\tvalid's binary_logloss: 0.00201125\n",
      "[10:27:28] [1200]\tvalid's binary_logloss: 0.0019331\n",
      "[10:27:28] [1300]\tvalid's binary_logloss: 0.00190772\n",
      "[10:27:28] [1400]\tvalid's binary_logloss: 0.00190306\n",
      "[10:27:28] [1500]\tvalid's binary_logloss: 0.00190051\n",
      "[10:27:28] Early stopping, best iteration is:\n",
      "[1350]\tvalid's binary_logloss: 0.00189496\n",
      "[10:27:28] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:27:28] Training until validation scores don't improve for 200 rounds\n",
      "[10:27:28] [100]\tvalid's binary_logloss: 0.239828\n",
      "[10:27:28] [200]\tvalid's binary_logloss: 0.110564\n",
      "[10:27:28] [300]\tvalid's binary_logloss: 0.0533175\n",
      "[10:27:28] [400]\tvalid's binary_logloss: 0.027057\n",
      "[10:27:28] [500]\tvalid's binary_logloss: 0.0144219\n",
      "[10:27:28] [600]\tvalid's binary_logloss: 0.00890564\n",
      "[10:27:28] [700]\tvalid's binary_logloss: 0.00591358\n",
      "[10:27:28] [800]\tvalid's binary_logloss: 0.0045565\n",
      "[10:27:28] [900]\tvalid's binary_logloss: 0.00390585\n",
      "[10:27:28] [1000]\tvalid's binary_logloss: 0.00366401\n",
      "[10:27:28] [1100]\tvalid's binary_logloss: 0.00354912\n",
      "[10:27:28] [1200]\tvalid's binary_logloss: 0.00346128\n",
      "[10:27:28] [1300]\tvalid's binary_logloss: 0.00344788\n",
      "[10:27:28] [1400]\tvalid's binary_logloss: 0.00345295\n",
      "[10:27:28] Early stopping, best iteration is:\n",
      "[1278]\tvalid's binary_logloss: 0.00343055\n",
      "[10:27:28] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-0.0027238698890078975\u001b[0m\n",
      "[10:27:28] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[10:27:28] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m ...\n",
      "[10:27:28] Training params: {'task_type': 'GPU', 'thread_count': 4, 'random_seed': 42, 'num_trees': 500, 'learning_rate': 0.02, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False, 'devices': '0'}\n",
      "[10:27:28] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:27:28] 0:\tlearn: 0.6250509\ttest: 0.6254426\tbest: 0.6254426 (0)\ttotal: 2.31ms\tremaining: 1.15s\n",
      "[10:27:28] 100:\tlearn: 0.0063827\ttest: 0.0190518\tbest: 0.0189323 (84)\ttotal: 176ms\tremaining: 697ms\n",
      "[10:27:29] bestTest = 0.01893234767\n",
      "[10:27:29] bestIteration = 84\n",
      "[10:27:29] Shrink model to first 85 iterations.\n",
      "[10:27:29] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:27:29] 0:\tlearn: 0.6291604\ttest: 0.6287864\tbest: 0.6287864 (0)\ttotal: 2.08ms\tremaining: 1.04s\n",
      "[10:27:29] 100:\tlearn: 0.0056396\ttest: 0.0099594\tbest: 0.0099295 (99)\ttotal: 176ms\tremaining: 694ms\n",
      "[10:27:30] 200:\tlearn: 0.0018013\ttest: 0.0103116\tbest: 0.0096580 (130)\ttotal: 352ms\tremaining: 523ms\n",
      "[10:27:30] bestTest = 0.009658037011\n",
      "[10:27:30] bestIteration = 130\n",
      "[10:27:30] Shrink model to first 131 iterations.\n",
      "[10:27:30] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:27:30] 0:\tlearn: 0.6281079\ttest: 0.6284958\tbest: 0.6284958 (0)\ttotal: 2.02ms\tremaining: 1.01s\n",
      "[10:27:30] 100:\tlearn: 0.0052521\ttest: 0.0111756\tbest: 0.0111756 (100)\ttotal: 178ms\tremaining: 704ms\n",
      "[10:27:31] 200:\tlearn: 0.0017821\ttest: 0.0111041\tbest: 0.0108165 (150)\ttotal: 353ms\tremaining: 525ms\n",
      "[10:27:31] bestTest = 0.01081646709\n",
      "[10:27:31] bestIteration = 150\n",
      "[10:27:31] Shrink model to first 151 iterations.\n",
      "[10:27:31] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:27:31] 0:\tlearn: 0.6298410\ttest: 0.6309551\tbest: 0.6309551 (0)\ttotal: 2.07ms\tremaining: 1.03s\n",
      "[10:27:32] 100:\tlearn: 0.0062200\ttest: 0.0215057\tbest: 0.0206236 (61)\ttotal: 173ms\tremaining: 682ms\n",
      "[10:27:32] bestTest = 0.02062361459\n",
      "[10:27:32] bestIteration = 61\n",
      "[10:27:32] Shrink model to first 62 iterations.\n",
      "[10:27:32] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:27:32] 0:\tlearn: 0.6295277\ttest: 0.6296191\tbest: 0.6296191 (0)\ttotal: 2.05ms\tremaining: 1.02s\n",
      "[10:27:32] 100:\tlearn: 0.0059288\ttest: 0.0109977\tbest: 0.0109920 (99)\ttotal: 173ms\tremaining: 682ms\n",
      "[10:27:33] 200:\tlearn: 0.0019023\ttest: 0.0093173\tbest: 0.0093173 (200)\ttotal: 344ms\tremaining: 511ms\n",
      "[10:27:33] 300:\tlearn: 0.0006629\ttest: 0.0094227\tbest: 0.0091825 (221)\ttotal: 514ms\tremaining: 340ms\n",
      "[10:27:33] bestTest = 0.009182462572\n",
      "[10:27:33] bestIteration = 221\n",
      "[10:27:33] Shrink model to first 222 iterations.\n",
      "[10:27:33] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m finished. score = \u001b[1m-0.013842634828328622\u001b[0m\n",
      "[10:27:33] \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m fitting and predicting completed\n",
      "[10:27:33] Time left 10791.28 secs\n",
      "\n",
      "[10:27:33] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[10:27:33] Blending: optimization starts with equal weights and score \u001b[1m-0.009652419036051657\u001b[0m\n",
      "[10:27:33] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m-0.0027238698890078975\u001b[0m, weights = \u001b[1m[0. 1. 0.]\u001b[0m\n",
      "[10:27:33] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m-0.0027238698890078975\u001b[0m, weights = \u001b[1m[0. 1. 0.]\u001b[0m\n",
      "[10:27:33] Blending: no score update. Terminated\n",
      "\n",
      "[10:27:33] \u001b[1mAutoml preset training completed in 8.75 seconds\u001b[0m\n",
      "\n",
      "[10:27:33] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# запускаем обучение на данных и подбор оптимальных параметров\n",
    "# из ансамбля моделей\n",
    "oof_pred = automl.fit_predict(tr_data, roles = roles, verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут некоторая тонкость использования метдов **automl.fit_predict()** и **automl.predict()**.\n",
    "\n",
    "Когда automl обучает модели методом **fit_predict()** это происходит с помощью кросс валидации (в нашем случае на 5 подвыборках из обучающей выборки). \n",
    "\n",
    "В процессе этого **automl** будет собирать статистику по фолдам, которые в данный момент являются валидационными и записывать ее.\n",
    "\n",
    "Поэтому и предсказанные значения по валидационным фолдам и полученные по ней метрики будут получены из метода automl.fit_predict(обучающая_выборка).\n",
    "\n",
    "automl.predict(данные) использует уже готовый полученный ансамбль моделей МЛ, некакого разбиения на подвыборки тут нет. Его праивльно использовать на тестовой выборке, - тех данных которые не участвовали при обучени модели.\n",
    "\n",
    "Предсказывать на automl.predict(обучающая_выборка) не правильно, таким образом мы померим метрику на тех данных на которых учили модель. А правильно делать это только на валлидационых фолдах при кросс валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним полученный ансамбль моделей в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/lightautoml_model_1.pkl'\n",
    "oof_pred_path = 'model/lightautoml_model_oof_pred_1.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/lightautoml_model_oof_pred_1.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохраним полученный ансамбль моделей и его предсказания на тестовой выборке\n",
    "joblib.dump(automl, model_path)\n",
    "joblib.dump(oof_pred, oof_pred_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим сохраненную ранее модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "porog = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = joblib.load(model_path)\n",
    "oof_pred = joblib.load(oof_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка полученного ансамбля на валидационной выборке\n",
    "valid_pred = automl.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = (valid_pred.data[:, 0] > porog).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([993, 400]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(valid_pred, return_counts=True)\n",
    "# porog = 0.5\n",
    "# (array([0, 1]), array([993, 400])) 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([993, 400]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(valid_data['target'].values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF f1_score: 0.998125\n",
      "VAL f1_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# оценка полученной метрики на валидационном наборе данных\n",
    "print(f\"OOF f1_score: {sk_metrics.f1_score(tr_data['target'].values, (oof_pred.data[:, 0] > porog).astype(int))}\")\n",
    "print(f\"VAL f1_score: {sk_metrics.f1_score(valid_data['target'].values, valid_pred)}\")\n",
    "\n",
    "# porog = 0.5\n",
    "# OOF f1_score: 0.9978077043532728\n",
    "# VAL f1_score: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_f1_score = []\n",
    "VAL_f1_score = []\n",
    "porogs = np.arange(0, 1, 0.1)\n",
    "\n",
    "for porog_i in porogs:\n",
    "    # проверка полученного ансамбля на валидационной выборке\n",
    "    valid_pred = automl.predict(valid_data)\n",
    "    valid_pred = (valid_pred.data[:, 0] > porog_i).astype(int)\n",
    "\n",
    "    # оценка полученной метрики на валидационном наборе данных\n",
    "    OOF_f1_score.append(sk_metrics.f1_score(tr_data['target'].values, (oof_pred.data[:, 0] > porog_i).astype(int)))\n",
    "    VAL_f1_score.append(sk_metrics.f1_score(valid_data['target'].values, valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "plt.plot(porogs, VAL_f1_score, '-r', label='OOF_f1_score')\n",
    "plt.plot(porogs, OOF_f1_score, '-b', label='VAL_f1_score')\n",
    "plt.xlabel('Пороги')\n",
    "plt.ylabel('f1_score')\n",
    "plt.xlim(0.1, 1)\n",
    "plt.ylim(0.99, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели на всей обучающей выборке и предсказание на тестовых данных (CatBoost) используем все"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task('binary', loss= 'logloss', metric = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {'target': 'target',\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(task = task, \n",
    "                    timeout = 3600 * 3, # 3600 секунд = 1 час\n",
    "                    general_params = {'use_algos': [['cb', 'lgb', 'linear_l2']]},\n",
    "                    cb_params = {'default_params': {'task_type': 'GPU'}},\n",
    "                    reader_params = {'n_jobs': 12},\n",
    "                    timing_params ={'mode': 0}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:29:23] Stdout logging level is DEBUG.\n",
      "[10:29:23] Task: binary\n",
      "\n",
      "[10:29:23] Start automl preset with listed constraints:\n",
      "[10:29:23] - time: 10800.00 seconds\n",
      "[10:29:23] - CPU: 4 cores\n",
      "[10:29:23] - memory: 16 GB\n",
      "\n",
      "[10:29:23] \u001b[1mTrain data shape: (6963, 32)\u001b[0m\n",
      "\n",
      "[10:29:23] Feats was rejected during automatic roles guess: []\n",
      "[10:29:23] Layer \u001b[1m1\u001b[0m train process start. Time left 10799.84 secs\n",
      "[10:29:23] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[10:29:23] Training params: {'tol': 1e-06, 'max_iter': 100, 'cs': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000], 'early_stopping': 2, 'categorical_idx': [], 'embed_sizes': (), 'data_size': 31}\n",
      "[10:29:23] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:29:23] Linear model: C = 1e-05 score = -0.5830472347039759\n",
      "[10:29:23] Linear model: C = 5e-05 score = -0.5275261467810766\n",
      "[10:29:23] Linear model: C = 0.0001 score = -0.47562432206755306\n",
      "[10:29:23] Linear model: C = 0.0005 score = -0.30863099075099676\n",
      "[10:29:23] Linear model: C = 0.001 score = -0.24420405777768958\n",
      "[10:29:23] Linear model: C = 0.005 score = -0.13848915414557755\n",
      "[10:29:23] Linear model: C = 0.01 score = -0.10908835233587887\n",
      "[10:29:23] Linear model: C = 0.05 score = -0.06403209191495654\n",
      "[10:29:23] Linear model: C = 0.1 score = -0.051085548883612275\n",
      "[10:29:23] Linear model: C = 0.5 score = -0.030036055324722293\n",
      "[10:29:23] Linear model: C = 1 score = -0.023702854025758505\n",
      "[10:29:23] Linear model: C = 5 score = -0.013575742418818358\n",
      "[10:29:23] Linear model: C = 10 score = -0.013575742418818358\n",
      "[10:29:23] Linear model: C = 50 score = -0.013575742418818358\n",
      "[10:29:23] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:29:23] Linear model: C = 1e-05 score = -0.583388828880395\n",
      "[10:29:23] Linear model: C = 5e-05 score = -0.5289979453405201\n",
      "[10:29:23] Linear model: C = 0.0001 score = -0.47822296560580835\n",
      "[10:29:23] Linear model: C = 0.0005 score = -0.314609905267609\n",
      "[10:29:23] Linear model: C = 0.001 score = -0.25135631178160067\n",
      "[10:29:23] Linear model: C = 0.005 score = -0.14498552299162798\n",
      "[10:29:23] Linear model: C = 0.01 score = -0.1149353653900205\n",
      "[10:29:23] Linear model: C = 0.05 score = -0.06700513738319092\n",
      "[10:29:23] Linear model: C = 0.1 score = -0.053229424141011464\n",
      "[10:29:23] Linear model: C = 0.5 score = -0.03152144548367936\n",
      "[10:29:23] Linear model: C = 1 score = -0.024993166969230967\n",
      "[10:29:23] Linear model: C = 5 score = -0.015375088563103385\n",
      "[10:29:23] Linear model: C = 10 score = -0.015375088563103385\n",
      "[10:29:23] Linear model: C = 50 score = -0.015375088563103385\n",
      "[10:29:23] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:29:23] Linear model: C = 1e-05 score = -0.5830682293596836\n",
      "[10:29:23] Linear model: C = 5e-05 score = -0.5280639335823333\n",
      "[10:29:23] Linear model: C = 0.0001 score = -0.476120559756737\n",
      "[10:29:23] Linear model: C = 0.0005 score = -0.30954552227891957\n",
      "[10:29:23] Linear model: C = 0.001 score = -0.24579847352652778\n",
      "[10:29:23] Linear model: C = 0.005 score = -0.1409570532819164\n",
      "[10:29:23] Linear model: C = 0.01 score = -0.11129120913853087\n",
      "[10:29:23] Linear model: C = 0.05 score = -0.06429718693375787\n",
      "[10:29:23] Linear model: C = 0.1 score = -0.05048279926869919\n",
      "[10:29:23] Linear model: C = 0.5 score = -0.02820118080838605\n",
      "[10:29:23] Linear model: C = 1 score = -0.02192401554963355\n",
      "[10:29:23] Linear model: C = 5 score = -0.012024912590919543\n",
      "[10:29:23] Linear model: C = 10 score = -0.012024912590919543\n",
      "[10:29:23] Linear model: C = 50 score = -0.012024912590919543\n",
      "[10:29:23] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:29:23] Linear model: C = 1e-05 score = -0.5832859970329479\n",
      "[10:29:23] Linear model: C = 5e-05 score = -0.5280498299191053\n",
      "[10:29:23] Linear model: C = 0.0001 score = -0.47619239082452897\n",
      "[10:29:23] Linear model: C = 0.0005 score = -0.3109331317941776\n",
      "[10:29:23] Linear model: C = 0.001 score = -0.24779598202674125\n",
      "[10:29:23] Linear model: C = 0.005 score = -0.1434106732594537\n",
      "[10:29:23] Linear model: C = 0.01 score = -0.11373200753754562\n",
      "[10:29:23] Linear model: C = 0.05 score = -0.06756528170247098\n",
      "[10:29:23] Linear model: C = 0.1 score = -0.05442723885975871\n",
      "[10:29:23] Linear model: C = 0.5 score = -0.03355619550471225\n",
      "[10:29:23] Linear model: C = 1 score = -0.0273858675873186\n",
      "[10:29:23] Linear model: C = 5 score = -0.018405776493903812\n",
      "[10:29:23] Linear model: C = 10 score = -0.018405776493903812\n",
      "[10:29:23] Linear model: C = 50 score = -0.018405776493903812\n",
      "[10:29:23] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[10:29:23] Linear model: C = 1e-05 score = -0.5830825677291415\n",
      "[10:29:23] Linear model: C = 5e-05 score = -0.5274831377954661\n",
      "[10:29:23] Linear model: C = 0.0001 score = -0.4753852881298497\n",
      "[10:29:23] Linear model: C = 0.0005 score = -0.30902346550090903\n",
      "[10:29:23] Linear model: C = 0.001 score = -0.24534947515850694\n",
      "[10:29:23] Linear model: C = 0.005 score = -0.14138822425668657\n",
      "[10:29:23] Linear model: C = 0.01 score = -0.11244925261910366\n",
      "[10:29:23] Linear model: C = 0.05 score = -0.06769113037358145\n",
      "[10:29:23] Linear model: C = 0.1 score = -0.054751983527433185\n",
      "[10:29:23] Linear model: C = 0.5 score = -0.033499686356387046\n",
      "[10:29:23] Linear model: C = 1 score = -0.02748277285090706\n",
      "[10:29:24] Linear model: C = 5 score = -0.017305736323473212\n",
      "[10:29:24] Linear model: C = 10 score = -0.017305736323473212\n",
      "[10:29:24] Linear model: C = 50 score = -0.017305736323473212\n",
      "[10:29:24] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m-0.015336727938928154\u001b[0m\n",
      "[10:29:24] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[10:29:24] Time left 10799.05 secs\n",
      "\n",
      "[10:29:24] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:24] [100]\tvalid's binary_logloss: 0.169085\n",
      "[10:29:24] [200]\tvalid's binary_logloss: 0.0603722\n",
      "[10:29:24] [300]\tvalid's binary_logloss: 0.0239406\n",
      "[10:29:24] [400]\tvalid's binary_logloss: 0.0108761\n",
      "[10:29:24] [500]\tvalid's binary_logloss: 0.00610659\n",
      "[10:29:24] [600]\tvalid's binary_logloss: 0.00433371\n",
      "[10:29:24] [700]\tvalid's binary_logloss: 0.00374612\n",
      "[10:29:24] [800]\tvalid's binary_logloss: 0.00353136\n",
      "[10:29:24] [900]\tvalid's binary_logloss: 0.00343521\n",
      "[10:29:24] [1000]\tvalid's binary_logloss: 0.00341521\n",
      "[10:29:24] [1100]\tvalid's binary_logloss: 0.003433\n",
      "[10:29:24] Early stopping, best iteration is:\n",
      "[937]\tvalid's binary_logloss: 0.00341247\n",
      "[10:29:24] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[10:29:24] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[10:29:24] Training params: {'task': 'train', 'learning_rate': 0.01, 'num_leaves': 32, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 0.5, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 4, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 200, 'random_state': 42}\n",
      "[10:29:24] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:29:24] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:24] [100]\tvalid's binary_logloss: 0.239264\n",
      "[10:29:24] [200]\tvalid's binary_logloss: 0.109893\n",
      "[10:29:24] [300]\tvalid's binary_logloss: 0.052412\n",
      "[10:29:24] [400]\tvalid's binary_logloss: 0.0263422\n",
      "[10:29:24] [500]\tvalid's binary_logloss: 0.0137805\n",
      "[10:29:24] [600]\tvalid's binary_logloss: 0.00823441\n",
      "[10:29:24] [700]\tvalid's binary_logloss: 0.00535489\n",
      "[10:29:24] [800]\tvalid's binary_logloss: 0.00391452\n",
      "[10:29:24] [900]\tvalid's binary_logloss: 0.00326554\n",
      "[10:29:24] [1000]\tvalid's binary_logloss: 0.00293814\n",
      "[10:29:24] [1100]\tvalid's binary_logloss: 0.00280307\n",
      "[10:29:24] [1200]\tvalid's binary_logloss: 0.00270227\n",
      "[10:29:24] [1300]\tvalid's binary_logloss: 0.00266829\n",
      "[10:29:24] [1400]\tvalid's binary_logloss: 0.00265753\n",
      "[10:29:24] [1500]\tvalid's binary_logloss: 0.00264769\n",
      "[10:29:24] [1600]\tvalid's binary_logloss: 0.00264115\n",
      "[10:29:24] [1700]\tvalid's binary_logloss: 0.00264115\n",
      "[10:29:24] Early stopping, best iteration is:\n",
      "[1527]\tvalid's binary_logloss: 0.00264115\n",
      "[10:29:24] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:29:24] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:24] [100]\tvalid's binary_logloss: 0.240407\n",
      "[10:29:24] [200]\tvalid's binary_logloss: 0.111432\n",
      "[10:29:24] [300]\tvalid's binary_logloss: 0.0539233\n",
      "[10:29:25] [400]\tvalid's binary_logloss: 0.0276987\n",
      "[10:29:25] [500]\tvalid's binary_logloss: 0.0150091\n",
      "[10:29:25] [600]\tvalid's binary_logloss: 0.00925768\n",
      "[10:29:25] [700]\tvalid's binary_logloss: 0.00627272\n",
      "[10:29:25] [800]\tvalid's binary_logloss: 0.004743\n",
      "[10:29:25] [900]\tvalid's binary_logloss: 0.00405816\n",
      "[10:29:25] [1000]\tvalid's binary_logloss: 0.00374413\n",
      "[10:29:25] [1100]\tvalid's binary_logloss: 0.00347458\n",
      "[10:29:25] [1200]\tvalid's binary_logloss: 0.00331247\n",
      "[10:29:25] [1300]\tvalid's binary_logloss: 0.00326483\n",
      "[10:29:25] [1400]\tvalid's binary_logloss: 0.00324413\n",
      "[10:29:25] [1500]\tvalid's binary_logloss: 0.00320548\n",
      "[10:29:25] [1600]\tvalid's binary_logloss: 0.00320548\n",
      "[10:29:25] Early stopping, best iteration is:\n",
      "[1472]\tvalid's binary_logloss: 0.00320125\n",
      "[10:29:25] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:29:25] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:25] [100]\tvalid's binary_logloss: 0.240851\n",
      "[10:29:25] [200]\tvalid's binary_logloss: 0.111665\n",
      "[10:29:25] [300]\tvalid's binary_logloss: 0.0541458\n",
      "[10:29:25] [400]\tvalid's binary_logloss: 0.0278291\n",
      "[10:29:25] [500]\tvalid's binary_logloss: 0.0151116\n",
      "[10:29:25] [600]\tvalid's binary_logloss: 0.00948472\n",
      "[10:29:25] [700]\tvalid's binary_logloss: 0.00652662\n",
      "[10:29:25] [800]\tvalid's binary_logloss: 0.00515987\n",
      "[10:29:25] [900]\tvalid's binary_logloss: 0.00458744\n",
      "[10:29:25] [1000]\tvalid's binary_logloss: 0.00435737\n",
      "[10:29:25] [1100]\tvalid's binary_logloss: 0.00425377\n",
      "[10:29:25] [1200]\tvalid's binary_logloss: 0.00414952\n",
      "[10:29:25] [1300]\tvalid's binary_logloss: 0.00416859\n",
      "[10:29:25] [1400]\tvalid's binary_logloss: 0.00411079\n",
      "[10:29:25] [1500]\tvalid's binary_logloss: 0.00413565\n",
      "[10:29:25] Early stopping, best iteration is:\n",
      "[1385]\tvalid's binary_logloss: 0.00411028\n",
      "[10:29:25] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:29:25] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:25] [100]\tvalid's binary_logloss: 0.238508\n",
      "[10:29:25] [200]\tvalid's binary_logloss: 0.108763\n",
      "[10:29:26] [300]\tvalid's binary_logloss: 0.0512023\n",
      "[10:29:26] [400]\tvalid's binary_logloss: 0.0250985\n",
      "[10:29:26] [500]\tvalid's binary_logloss: 0.0124971\n",
      "[10:29:26] [600]\tvalid's binary_logloss: 0.0069831\n",
      "[10:29:26] [700]\tvalid's binary_logloss: 0.00400727\n",
      "[10:29:26] [800]\tvalid's binary_logloss: 0.00264433\n",
      "[10:29:26] [900]\tvalid's binary_logloss: 0.00201681\n",
      "[10:29:26] [1000]\tvalid's binary_logloss: 0.00167493\n",
      "[10:29:26] [1100]\tvalid's binary_logloss: 0.00149617\n",
      "[10:29:26] [1200]\tvalid's binary_logloss: 0.00138266\n",
      "[10:29:26] [1300]\tvalid's binary_logloss: 0.00135306\n",
      "[10:29:26] [1400]\tvalid's binary_logloss: 0.0013237\n",
      "[10:29:26] [1500]\tvalid's binary_logloss: 0.00131265\n",
      "[10:29:26] [1600]\tvalid's binary_logloss: 0.00131265\n",
      "[10:29:26] Early stopping, best iteration is:\n",
      "[1477]\tvalid's binary_logloss: 0.00130552\n",
      "[10:29:26] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[10:29:26] Training until validation scores don't improve for 200 rounds\n",
      "[10:29:26] [100]\tvalid's binary_logloss: 0.240386\n",
      "[10:29:26] [200]\tvalid's binary_logloss: 0.110814\n",
      "[10:29:26] [300]\tvalid's binary_logloss: 0.0532791\n",
      "[10:29:26] [400]\tvalid's binary_logloss: 0.0270824\n",
      "[10:29:26] [500]\tvalid's binary_logloss: 0.0143667\n",
      "[10:29:26] [600]\tvalid's binary_logloss: 0.008694\n",
      "[10:29:26] [700]\tvalid's binary_logloss: 0.00563088\n",
      "[10:29:26] [800]\tvalid's binary_logloss: 0.00417822\n",
      "[10:29:26] [900]\tvalid's binary_logloss: 0.00357217\n",
      "[10:29:26] [1000]\tvalid's binary_logloss: 0.00317405\n",
      "[10:29:26] [1100]\tvalid's binary_logloss: 0.00294316\n",
      "[10:29:26] [1200]\tvalid's binary_logloss: 0.00268782\n",
      "[10:29:26] [1300]\tvalid's binary_logloss: 0.00258991\n",
      "[10:29:26] [1400]\tvalid's binary_logloss: 0.00251057\n",
      "[10:29:26] [1500]\tvalid's binary_logloss: 0.00249146\n",
      "[10:29:26] [1600]\tvalid's binary_logloss: 0.0024759\n",
      "[10:29:26] [1700]\tvalid's binary_logloss: 0.0024759\n",
      "[10:29:26] Early stopping, best iteration is:\n",
      "[1553]\tvalid's binary_logloss: 0.00247547\n",
      "[10:29:26] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-0.0027553863943531416\u001b[0m\n",
      "[10:29:26] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[10:29:26] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m ...\n",
      "[10:29:26] Training params: {'task_type': 'GPU', 'thread_count': 4, 'random_seed': 42, 'num_trees': 5000, 'learning_rate': 0.035, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False, 'devices': '0'}\n",
      "[10:29:26] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:29:26] 0:\tlearn: 0.5851653\ttest: 0.5860378\tbest: 0.5860378 (0)\ttotal: 2.13ms\tremaining: 10.7s\n",
      "[10:29:27] 100:\tlearn: 0.0031227\ttest: 0.0162231\tbest: 0.0152434 (45)\ttotal: 177ms\tremaining: 8.59s\n",
      "[10:29:27] bestTest = 0.01524344538\n",
      "[10:29:27] bestIteration = 45\n",
      "[10:29:27] Shrink model to first 46 iterations.\n",
      "[10:29:27] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:29:27] 0:\tlearn: 0.5866578\ttest: 0.5874382\tbest: 0.5874382 (0)\ttotal: 2.01ms\tremaining: 10.1s\n",
      "[10:29:28] 100:\tlearn: 0.0039144\ttest: 0.0196453\tbest: 0.0184796 (41)\ttotal: 177ms\tremaining: 8.57s\n",
      "[10:29:28] bestTest = 0.01847959848\n",
      "[10:29:28] bestIteration = 41\n",
      "[10:29:28] Shrink model to first 42 iterations.\n",
      "[10:29:28] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:29:28] 0:\tlearn: 0.5877485\ttest: 0.5870070\tbest: 0.5870070 (0)\ttotal: 2.13ms\tremaining: 10.7s\n",
      "[10:29:28] 100:\tlearn: 0.0034392\ttest: 0.0082136\tbest: 0.0081843 (98)\ttotal: 177ms\tremaining: 8.59s\n",
      "[10:29:29] 200:\tlearn: 0.0007461\ttest: 0.0093411\tbest: 0.0077462 (115)\ttotal: 351ms\tremaining: 8.38s\n",
      "[10:29:29] bestTest = 0.007746165892\n",
      "[10:29:29] bestIteration = 115\n",
      "[10:29:29] Shrink model to first 116 iterations.\n",
      "[10:29:29] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:29:29] 0:\tlearn: 0.5849176\ttest: 0.5855702\tbest: 0.5855702 (0)\ttotal: 2.23ms\tremaining: 11.1s\n",
      "[10:29:30] 100:\tlearn: 0.0035142\ttest: 0.0152514\tbest: 0.0150462 (90)\ttotal: 178ms\tremaining: 8.64s\n",
      "[10:29:30] bestTest = 0.01504615806\n",
      "[10:29:30] bestIteration = 90\n",
      "[10:29:30] Shrink model to first 91 iterations.\n",
      "[10:29:30] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m =====\n",
      "[10:29:30] 0:\tlearn: 0.5787404\ttest: 0.5783921\tbest: 0.5783921 (0)\ttotal: 2.12ms\tremaining: 10.6s\n",
      "[10:29:31] 100:\tlearn: 0.0031629\ttest: 0.0138146\tbest: 0.0138146 (100)\ttotal: 176ms\tremaining: 8.54s\n",
      "[10:29:31] 200:\tlearn: 0.0006765\ttest: 0.0154526\tbest: 0.0137053 (114)\ttotal: 350ms\tremaining: 8.36s\n",
      "[10:29:31] bestTest = 0.01370528375\n",
      "[10:29:31] bestIteration = 114\n",
      "[10:29:31] Shrink model to first 115 iterations.\n",
      "[10:29:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m finished. score = \u001b[1m-0.014044085033245852\u001b[0m\n",
      "[10:29:31] \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m fitting and predicting completed\n",
      "[10:29:31] Time left 10791.83 secs\n",
      "\n",
      "[10:29:31] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[10:29:31] Blending: optimization starts with equal weights and score \u001b[1m-0.009071361131804543\u001b[0m\n",
      "[10:29:31] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m-0.0027553863943531416\u001b[0m, weights = \u001b[1m[0. 1. 0.]\u001b[0m\n",
      "[10:29:31] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m-0.0027553863943531416\u001b[0m, weights = \u001b[1m[0. 1. 0.]\u001b[0m\n",
      "[10:29:31] Blending: no score update. Terminated\n",
      "\n",
      "[10:29:31] \u001b[1mAutoml preset training completed in 8.21 seconds\u001b[0m\n",
      "\n",
      "[10:29:31] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучение этим подходом на всех имеющиеся обучающих данных\n",
    "oof_pred = automl.fit_predict(train_data, roles = roles, verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним полученный ансамбль моделей в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/lightautoml_model_final_2.pkl'\n",
    "oof_pred_path = 'model/lightautoml_model_oof_pred_final_2.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/lightautoml_model_oof_pred_final_2.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохраним полученный ансамбль моделей и его предсказания на тестовой выборке\n",
    "joblib.dump(automl, model_path)\n",
    "joblib.dump(oof_pred, oof_pred_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим сохраненую ранее модель из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = joblib.load(model_path)\n",
    "oof_pred = joblib.load(oof_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL f1_score: 0.99676052828308\n"
     ]
    }
   ],
   "source": [
    "# оценка полученной метрики на всем наборе данных\n",
    "print(f\"FULL f1_score: {sk_metrics.f1_score(train_data['target'].values, (oof_pred.data[:, 0] > porog).astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предскажем на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказание ансамблем моделей на тестовых данных\n",
    "test_pred = automl.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем файл sabmita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = 'submission/lightautoml_model_final_submission_2_porog_03.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = (test_pred.data[:, 0] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(submission_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# совпадают ли количество рядов у таблицы нашего предсказания\n",
    "# и \"sample_submission.csv\"\n",
    "len(submission)==len(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Краткие выводы:**\n",
    "* смена задачи на task = Task('binary', loss= 'logloss', metric = 'logloss') сразу дала масимально положительный эффект на LGB\n",
    "кажется объяснение этому есть тут!\n",
    "https://dyakonov.org/2021/05/27/imbalance/\n",
    "\n",
    "* подбор оптимального порога интересная вещь, в этом соревновании разницы нет никакой. Дело в чем то еще ! =)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sber_lama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "901158d8cd2db934fb9192b2a062c20c24f00bed87efea9fcd6e00355046a3d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
